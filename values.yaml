#
# The Alluxio Open Foundation licenses this work under the Apache License, version 2.0
# (the "License"). You may not use this work except in compliance with the License, which is
# available at www.apache.org/licenses/LICENSE-2.0
#
# This software is distributed on an "AS IS" basis, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied, as more fully set forth in the License.
#
# See the NOTICE file distributed with this work for information regarding copyright ownership.
#

# This should not be modified in the usual case.
fullnameOverride: alluxio

s3:
  bucket: s3://{bucket_name}
  endpoint:
    host:
    region:
  accessKey:
  secretKey:



## Common ##

# Docker Image
alluxio:
  image: alluxio/alluxio
  imageTag: 2.7.2
  imagePullPolicy: IfNotPresent

# Security Context
user: 1000
group: 1000
fsGroup: 1000

# Service Account
#   If not specified, Kubernetes will assign the 'default'
#   ServiceAccount used for the namespace
serviceAccount:

# Image Pull Secret
#   The secrets will need to be created externally from
#   this Helm chart, but you can configure the Alluxio
#   Pods to use the following list of secrets
# eg:
# imagePullSecrets:
#   - ecr
#   - dev
imagePullSecrets:

# Site properties for all the components
properties:
  # alluxio.user.metrics.collection.enabled: 'true'
  alluxio.security.stale.channel.purge.interval: 365d
  alluxio.hub.manager.rpc.hostname: alluxio-hub

# Recommended JVM Heap options for running in Docker
# Ref: https://developers.redhat.com/blog/2017/03/14/java-inside-docker/
# These JVM options are common to all Alluxio services
# jvmOptions:
#   - "-XX:+UnlockExperimentalVMOptions"
#   - "-XX:+UseCGroupMemoryLimitForHeap"
#   - "-XX:MaxRAMFraction=2"

# Mount Persistent Volumes to all components
# mounts:
# - name: <persistentVolume claimName>
#   path: <mountPath>

# Use labels to run Alluxio on a subset of the K8s nodes
# nodeSelector: {}

# A list of K8s Node taints to allow scheduling on.
# See the Kubernetes docs for more info:
# - https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
# eg: tolerations: [ {"key": "env", "operator": "Equal", "value": "prod", "effect": "NoSchedule"} ]
# tolerations: []

## Master ##

master:
  enabled: true
  hostname: alluxio-master-0
  count: 1 # Controls the number of StatefulSets. For multiMaster mode increase this to >1.
  replicas: 1 # Controls #replicas in a StatefulSet and should not be modified in the usual case.
  env:
    # Extra environment variables for the master pod
    # Example:
    # JAVA_HOME: /opt/java
  args: # Arguments to Docker entrypoint
    - master-only
    - --no-format
  config:
    path: /conf
    subPath: alluxio-site.properties
  # Properties for the master component
  properties:
    # Example: use ROCKS DB instead of Heap
    # alluxio.master.metastore: ROCKS
    # alluxio.master.metastore.dir: /metastore

  resources:
    # The default xmx is 8G
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 19998
    web: 19999
    embedded: 19200
  hostPID: false
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to the master container
  jvmOptions:
  nodeSelector: {}
  # When using HA Alluxio masters, the expected startup time
  # can take over 2-3 minutes (depending on leader elections,
  # journal catch-up, etc). In that case it is recommended
  # to allow for up to at least 3 minutes with the readinessProbe,
  # though higher values may be desired for some leniancy.
  # - Note that the livenessProbe does not wait for the
  #   readinessProbe to succeed first
  #
  # eg: 3 minute startupProbe and readinessProbe
  # readinessProbe:
  #   initialDelaySeconds: 30
  #   periodSeconds: 10
  #   timeoutSeconds: 1
  #   failureThreshold: 15
  #   successThreshold: 3
  # startupProbe:
  #   initialDelaySeconds: 60
  #   periodSeconds: 30
  #   timeoutSeconds: 5
  #   failureThreshold: 4
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  # If you are using Kubernetes 1.18+ or have the feature gate
  # for it enabled, use startupProbe to prevent the livenessProbe
  # from running until the startupProbe has succeeded
  # startupProbe:
  #   initialDelaySeconds: 15
  #   periodSeconds: 30
  #   timeoutSeconds: 5
  #   failureThreshold: 2
  tolerations: []
  podAnnotations: {}
  # The ServiceAccount provided here will have precedence over
  # the global `serviceAccount`
  serviceAccount:

jobMaster:
  args:
    - job-master
  # Properties for the jobMaster component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 20001
    web: 20002
    embedded: 20003
  # JVM options specific to the jobMaster container
  jvmOptions:
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  # If you are using Kubernetes 1.18+ or have the feature gate
  # for it enabled, use startupProbe to prevent the livenessProbe
  # from running until the startupProbe has succeeded
  # startupProbe:
  #   initialDelaySeconds: 15
  #   periodSeconds: 30
  #   timeoutSeconds: 5
  #   failureThreshold: 2

# Alluxio supports journal type of UFS and EMBEDDED
# UFS journal with HDFS example
# journal:
#   type: "UFS"
#   folder: "hdfs://{$hostname}:{$hostport}/journal"
# EMBEDDED journal to /journal example
# journal:
#   type: "EMBEDDED"
#   folder: "/journal"
journal:
  type: "UFS" # "UFS" or "EMBEDDED"
  ufsType: "local" # Ignored if type is "EMBEDDED". "local" or "HDFS"
  folder: "/journal" # Master journal folder
  # volumeType controls the type of journal volume.
  # It can be "persistentVolumeClaim" or "emptyDir"
  volumeType: persistentVolumeClaim
  size: 1Gi
  # Attributes to use when the journal is persistentVolumeClaim
  storageClass: "standard"
  accessModes:
    - ReadWriteOnce
  # Attributes to use when the journal is emptyDir
  medium: ""
  # Configuration for journal formatting job
  format:
    runFormat: false # Change to true to format journal


# You can enable metastore to use ROCKS DB instead of Heap
# metastore:
#   volumeType: persistentVolumeClaim # Options: "persistentVolumeClaim" or "emptyDir"
#   size: 1Gi
#   mountPath: /metastore
# # Attributes to use when the metastore is persistentVolumeClaim
#   storageClass: "standard"
#   accessModes:
#    - ReadWriteOnce
# # Attributes to use when the metastore is emptyDir
#   medium: ""


## Worker ##

worker:
  enabled: true
  hostname: alluxio-worker-0
  count: 1
  user: 0
  env:
    # Extra environment variables for the worker pod
    # Example:
    # JAVA_HOME: /opt/java
  args:
    - worker-only
    - --no-format
  # Properties for the worker component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "4G"
    requests:
      cpu: "1"
      memory: "2G"
  ports:
    rpc: 29999
    web: 30000
  # hostPID requires escalated privileges
  hostPID: false
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to the worker container
  jvmOptions:
  nodeSelector: {}
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  # If you are using Kubernetes 1.18+ or have the feature gate
  # for it enabled, use startupProbe to prevent the livenessProbe
  # from running until the startupProbe has succeeded
  # startupProbe:
  #   initialDelaySeconds: 15
  #   periodSeconds: 30
  #   timeoutSeconds: 5
  #   failureThreshold: 2
  tolerations: []
  podAnnotations: {}
  # The ServiceAccount provided here will have precedence over
  # the global `serviceAccount`
  serviceAccount:
  # Setting fuseEnabled to true will embed Fuse in worker process. The worker pods will
  # launch the Alluxio workers using privileged containers with `SYS_ADMIN` capability.
  # Be sure to give root access to the pod by setting the global user/group/fsGroup
  # values to `0` to turn on Fuse in worker.
  fuseEnabled: false

jobWorker:
  args:
    - job-worker
  # Properties for the jobWorker component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "4G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 30001
    data: 30002
    web: 30003
  # JVM options specific to the jobWorker container
  jvmOptions:
  readinessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 2
  # If you are using Kubernetes 1.18+ or have the feature gate
  # for it enabled, use startupProbe to prevent the livenessProbe
  # from running until the startupProbe has succeeded
  # startupProbe:
  #   initialDelaySeconds: 15
  #   periodSeconds: 30
  #   timeoutSeconds: 5
  #   failureThreshold: 2

# Tiered Storage
# emptyDir example
#  - level: 0
#    alias: MEM
#    mediumtype: MEM
#    path: /dev/shm
#    type: emptyDir
#    quota: 1G
#
# hostPath example
#  - level: 0
#    alias: MEM
#    mediumtype: MEM
#    path: /dev/shm
#    type: hostPath
#    quota: 1G
#
# persistentVolumeClaim example
#  - level: 1
#    alias: SSD
#    mediumtype: SSD
#    type: persistentVolumeClaim
#    name: alluxio-ssd
#    path: /dev/ssd
#    quota: 10G
#
# multi-part mediumtype example
#  - level: 1
#    alias: SSD,HDD
#    mediumtype: SSD,HDD
#    type: persistentVolumeClaim
#    name: alluxio-ssd,alluxio-hdd
#    path: /dev/ssd,/dev/hdd
#    quota: 10G,10G
tieredstore:
  levels:
  - level: 0
    alias: MEM
    mediumtype: MEM
    path: /dev/shm
    type: emptyDir
    quota: 1G
    high: 0.95
    low: 0.7

# Short circuit related properties
shortCircuit:
  enabled: true
  # The policy for short circuit can be "local" or "uuid",
  # local means the cache directory is in the same mount namespace,
  # uuid means interact with domain socket
  policy: uuid
  # volumeType controls the type of shortCircuit volume.
  # It can be "persistentVolumeClaim" or "hostPath"
  volumeType: persistentVolumeClaim
  size: 1Mi
  # Attributes to use if the domain socket volume is PVC
  pvcName: alluxio-worker-domain-socket
  accessModes:
    - ReadWriteOnce
  storageClass: standard
  # Attributes to use if the domain socket volume is hostPath
  hostPath: "/tmp/alluxio-domain" # The hostPath directory to use


## FUSE ##

fuse:
  env:
    # Extra environment variables for the fuse pod
    # Example:
    # JAVA_HOME: /opt/java
  # Change both to true to deploy FUSE
  enabled: false
  clientEnabled: false
  # Properties for the fuse component
  properties:
  # Customize the MaxDirectMemorySize
  # These options are specific to the FUSE daemon
  jvmOptions:
    - "-XX:MaxDirectMemorySize=2g"
  hostNetwork: true
  # hostPID requires escalated privileges
  hostPID: true
  dnsPolicy: ClusterFirstWithHostNet
  user: 0
  group: 0
  fsGroup: 0
  # fuse args specific to standalone fuse
  args:
    - fuse
    - --fuse-opts=allow_other
  # fuse options specific to fuse embedded in the worker process, separated by comma without whitespace
  mountOptions: allow_other
  # Mount path in the host
  mountPath: /mnt/alluxio-fuse
  # The path in Alluxio being mounted
  alluxioPath: /
  resources:
    requests:
      cpu: "0.5"
      memory: "1G"
    limits:
      cpu: "4"
      memory: "4G"
  nodeSelector: {}
  tolerations: []
  podAnnotations: {}
  # The ServiceAccount provided here will have precedence over
  # the global `serviceAccount`
  serviceAccount:


##  Secrets ##

# Format: (<name>:<mount path under /secrets/>):
# secrets:
#   master: # Shared by master and jobMaster containers
#     alluxio-hdfs-config: hdfsConfig
#   worker: # Shared by worker and jobWorker containers
#     alluxio-hdfs-config: hdfsConfig
#   logserver: # Used by the logserver container
#     alluxio-hdfs-config: hdfsConfig

##  Metrics System ##

# Settings for Alluxio metrics. Disabled by default.
metrics:
  enabled: false
  # Enable ConsoleSink by class name
  ConsoleSink:
    enabled: false
    # Polling period for ConsoleSink
    period: 10
    # Unit of poll period
    unit: seconds
  # Enable CsvSink by class name
  CsvSink:
    enabled: false
    # Polling period for CsvSink
    period: 1
    # Unit of poll period
    unit: seconds
    # Polling directory for CsvSink, ensure this directory exists!
    directory: /tmp/alluxio-metrics
  # Enable JmxSink by class name
  JmxSink:
    enabled: false
    # Jmx domain
    domain: org.alluxio
  # Enable GraphiteSink by class name
  GraphiteSink:
    enabled: false
    # Hostname of Graphite server
    host: NONE
    # Port of Graphite server
    port: NONE
    # Poll period
    period: 10
    # Unit of poll period
    unit: seconds
    # Prefix to prepend to metric name
    prefix: ""
  # Enable Slf4jSink by class name
  Slf4jSink:
    enabled: false
    # Poll period
    period: 10
    # Units of poll period
    unit: seconds
    # Contains all metrics
    filterClass: null
    # Contains all metrics
    filterRegex: null
  # Enable PrometheusMetricsServlet by class name
  PrometheusMetricsServlet:
    enabled: false
  # Pod annotations for Prometheus
  # podAnnotations:
  #   prometheus.io/scrape: "true"
  #   prometheus.io/port: "19999"
  #   prometheus.io/path: "/metrics/prometheus/"
  podAnnotations: {}

# Remote logging server
logserver:
  enabled: false
  replicas: 1
  env:
  # Extra environment variables for the logserver pod
  # Example:
  # JAVA_HOME: /opt/java
  args: # Arguments to Docker entrypoint
    - logserver
  # Properties for the logserver component
  properties:
  resources:
    # The default xmx is 8G
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    logging: 45600
  hostPID: false
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to the logserver container
  jvmOptions:
  nodeSelector: {}
  tolerations: []
  # The strategy field corresponds to the .spec.strategy field for the deployment
  # This specifies the strategy used to replace old Pods by new ones
  # https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  # The default is Recreate which kills the existing Pod before creating a new one
  # Note: When using RWO PVCs, the strategy MUST be Recreate, because the PVC cannot
  # be passed from the old Pod to the new one
  # When using RWX PVCs, you can use RollingUpdate strategy to ensure zero down time
  # Example:
  # strategy:
  #   type: RollingUpdate
  #   rollingUpdate:
  #     maxUnavailable: 25%
  #     maxSurge: 1
  strategy:
    type: Recreate
  # volumeType controls the type of log volume.
  # It can be "persistentVolumeClaim" or "hostPath" or "emptyDir"
  volumeType: persistentVolumeClaim
  # Attributes to use if the log volume is PVC
  pvcName: alluxio-logserver-logs
  # Note: If using RWO, the strategy MUST be Recreate
  # If using RWX, the strategy can be RollingUpdate
  accessModes:
    - ReadWriteOnce
  storageClass: standard
  # If you are dynamically provisioning PVs, the selector on the PVC should be empty.
  # Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class-1
  selector: {}
  # If you are manually allocating PV for the logserver,
  # it is recommended to use selectors to make sure the PV and PVC match as expected.
  # You can specify selectors like below:
  # Example:
  # selector:
  #   matchLabels:
  #     role: alluxio-logserver
  #     app: alluxio
  #     chart: alluxio-<chart version>
  #     release: alluxio
  #     heritage: Helm
  #     dc: data-center-1
  #     region: us-east

  # Attributes to use if the log volume is hostPath
  hostPath: "/tmp/alluxio-logs" # The hostPath directory to use
  # Attributes to use when the log volume is emptyDir
  medium: ""
  size: 4Gi

# The pod's HostAliases. HostAliases is an optional list of hosts and IPs that will be injected into the pod's hosts file if specified.
# It is mainly to provide the external host addresses for services not in the K8s cluster, like HDFS.
# Example:
# hostAliases:
# - ip: "192.168.0.1"
#   hostnames:
#     - "example1.com"
#     - "example2.com"

# kubernetes CSI plugin
csi:
  enabled: false
  imagePullPolicy: IfNotPresent
  controllerPlugin:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet
    provisioner:
      # for kubernetes 1.17 or above
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.5
      resources:
        limits:
          cpu: 100m
          memory: 300Mi
        requests:
          cpu: 10m
          memory: 20Mi
    controller:
      resources:
        limits:
          cpu: 200m
          memory: 200Mi
        requests:
          cpu: 10m
          memory: 20Mi
  # Will run fuse daemon inside csi nodeserver
  nodePlugin:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet
    nodeserver:
      resources:
        # The default xmx is 8G
        limits:
          cpu: "4"
          memory: "8G"
        requests:
          cpu: "1"
          memory: "1G"
    driverRegistrar:
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.0
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 10m
          memory: 20Mi

  # for csi client
  clientEnabled: false
  accessModes:
    - ReadWriteMany
  quota: 100Gi
  mountPath: /data
  alluxioPath: /
  mountOptions:
    - kernel_cache
    - allow_other
    - entry_timeout=36000
    - attr_timeout=36000
    - max_readahead=0
  javaOptions: "-Dalluxio.user.metadata.cache.enabled=true "

hub:
  enabled: false
  env:
  # Example:
  # JAVA_HOME: /opt/java
  args: # Arguments to Docker entrypoint
    - hub-manager
  resources:
    limits:
      cpu: "1"
      memory: "1G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    web: 30077
    rpc: 30076
  agent:
    args: # Arguments to Docker entrypoint
      - hub-agent
    ports:
      rpc: 30075
  serviceAccount: default


server:
  workers: 4
  node:
    environment: production
    dataDir: /data/trino
    pluginDir: /data/trino/plugin
  log:
    trino:
      level: INFO
  config:
    path: /etc/trino
    pathCatalog: /etc/trino/catalog
    http:
      port: 8080
    https:
      enabled: true
      port: 8443
    query:
      maxMemory: "20GB"
      maxTotalMemory: "32GB"
      maxMemoryPerNode: "6GB"
      maxTotalMemoryPerNode: "10GB"
  jvm:
    maxHeapSize: "24G"
    gcMethod:
      type: "UseG1GC"
      g1:
        heapRegionSize: "32M"
  jmx:
    registryPort: 9080
    serverPort: 9081

image:
  repository: trinodb/trino
  tag: 356
  pullPolicy: IfNotPresent
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000


service:
  type: ClusterIP
  externalType: NodePort

resources:
  coordinator:
    limits:
      cpu: 3000m
      memory: 30000Mi
    requests:
      cpu: 2000m
      memory: 24000Mi
  worker:
    limits:
      cpu: 2000m
      memory: 30000Mi
    requests:
      cpu: 1000m
      memory: 28000Mi

nodeSelector: {}

tolerations: []

affinity: {}

hive:
  metastore:
    uri: thrift://metastore-headless:9083
  s3:
    endpoint: https://host:port
    ssl:
      enabled: true
    max-connections: 100
storagegrid:
  secret: dma-dev-storagegrid-access-key
  accessKey:
  secretKey:
passwordfile:
  secret: trino-passworddb
  mountpath: /usr/lib/trino/etc
  filepath: /usr/lib/trino/etc/password.db
keystore:
  secret: trino-keystore
  mountpath: /usr/lib/trino/etc/keystore
  filepath: /usr/lib/trino/etc/keystore/keystore.jks
  key: key
exporters:
  jmx:
    enabled: true
    image:
      repository: sscaling/jmx-prometheus-exporter
      tag: 0.3.0
      pullPolicy: IfNotPresent
    port: 9999
    env: {}
    resources:
      coordinator:
        limits:
          cpu: 3000m
          memory: 10000Mi
        requests:
          cpu: 1000m
          memory: 8000Mi
      worker:
        limits:
          cpu: 1000m
          memory: 3000Mi
        requests:
          cpu: 800m
          memory: 2000Mi

    config:
      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:9080/jmxrmi
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true

      whitelistObjectNames:
      - 'trino.execution:*'
      - 'trino.execution.executor:*'
      - 'trino.execution.resourcegroups:*'
      - 'trino.failuredetector:*'
      - 'trino.memory:*'
      - 'trino.metadata:*'
      - 'trino.security:*'
      - 'trino.sql.gen:*'
      - 'trino.sql.planner.iterative:*'
      - 'trino.sql.planner.optimizations:*'
      rules:
        # Trino execution stats {{{
        - pattern: 'trino.execution<name=ClusterSizeMonitor><>RequiredWorkers'
          name: 'trino_cluster_size_monitor_required_workers'
          help: 'Number of workers required in cluster'
          type: GAUGE

        # Query counts {{{
        - pattern: 'trino.execution<name=QueryManager><>(Abandoned|Canceled|Completed|Failed|Started|Submitted)Queries.TotalCount'
          name: 'trino_query_manager_queries_total'
          help: 'Total number of queries'
          type: COUNTER
          labels:
            state: '$1'
        - pattern: 'trino.execution<name=QueryManager><>(Queued|Running)Queries'
          name: 'trino_query_manager_queries'
          help: 'Number of queries currently in the labelled state'
          type: GAUGE
          labels:
            state: '$1'
        # }}}

        - pattern: 'trino.execution<name=QueryManager><>ConsumedCpuTimeSecs.TotalCount'
          name: 'trino_query_manager_consumed_cpu_time_seconds_total'
          help: 'Total CPU time consumed across all queries in seconds'
          type: COUNTER
        - pattern: 'trino.execution<name=QueryManager><>ConsumedInputBytes.TotalCount'
          name: 'trino_query_manager_consumed_input_bytes_total'
          help: 'Total input bytes consumed across all queries in bytes'
          type: COUNTER
        - pattern: 'trino.execution<name=QueryManager><>ConsumedInputRows.TotalCount'
          name: 'trino_query_manager_consumed_input_rows_total'
          help: 'Total number of input rows consumed across all queries'
          type: COUNTER

        # CPU and Wall input byte rate histograms/summaries {{{
        - pattern: 'trino.execution<name=QueryManager><>(CpuInputByte|WallInputBytes)Rate.AllTime.(Avg|Min|Max)'
          name: 'trino_query_manager_$1_rate_$2'
          help: '$2 rate $1'
          type: GAUGE
        - pattern: 'trino.execution<name=QueryManager><>(CpuInputByte|WallInputBytes)Rate.AllTime.P(\d+)'
          name: 'trino_query_manager_$1_rate'
          help: '$1 rate for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution<name=QueryManager><>(CpuInputByte|WallInputBytes)Rate.AllTime.Count'
          name: 'trino_query_manager_$1_rate_count'
          help: '$1 rate count'
          type: COUNTER
        # }}}

        # Execution and queued time histograms/summaries {{{
        - pattern: 'trino.execution<name=QueryManager><>(Execution|Queued)Time.AllTime.(Avg|Min|Max)'
          valueFactor: 0.001
          name: 'trino_query_manager_$1_seconds_$2'
          help: '$2 $1 time in seconds'
          type: GAUGE
        - pattern: 'trino.execution<name=QueryManager><>(Execution|Queued)Time.AllTime.P(\d+)'
          valueFactor: 0.001
          name: 'trino_query_manager_$1_seconds'
          help: '$1 time in seconds for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution<name=QueryManager><>(Execution|Queued)Time.AllTime.Count'
          valueFactor: 0.001
          name: 'trino_query_manager_$1_seconds_count'
          help: '$1 time count'
          type: COUNTER
        # }}}

        - pattern: 'trino.execution<name=QueryManager><>(External|InsufficientResources|Internal|UserError)Failures.TotalCount'
          name: 'trino_query_manager_failures_total'
          help: 'Total number of failures with the labelled type'
          type: COUNTER
          labels:
            type: '$1'
        # }}}

        # Trino executor stats {{{
        # MLFQ stats {{{
        - pattern: 'trino.execution.executor<name=MultilevelSplitQueue><>Level(\d)Time'
          valueFactor: 0.000000001
          name: 'trino_multilevel_split_queue_level_time_seconds_total'
          help: 'Total time spent in the labelled level of the MLFQ in seconds'
          type: COUNTER
          labels:
            level: '$1'
        - pattern: 'trino.execution.executor<name=MultilevelSplitQueue><>SelectedCountLevel(\d).TotalCount'
          name: 'trino_multilevel_split_queue_level_selected_total'
          help: 'Total number of times the labelled level was selected'
          type: COUNTER
          labels:
            level: '$1'
        # }}}

        # BlockedQuantaWallTime, SplitQueuedTime, SplitWallTime and UnblockedQuantaWallTime histograms {{{
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(BlockedQuantaWallTime|SplitQueuedTime|SplitWallTime|UnblockedQuantaWallTime).AllTime.(Avg|Min|Max)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_$2'
          help: '$2 $1 in seconds'
          type: GAUGE
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(BlockedQuantaWallTime|SplitQueuedTime|SplitWallTime|UnblockedQuantaWallTime).AllTime.P(\d+)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds'
          help: '$1 in seconds for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(BlockedQuantaWallTime|SplitQueuedTime|SplitWallTime|UnblockedQuantaWallTime).AllTime.Count'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_count'
          help: '$1 in seconds count'
          type: COUNTER
        # }}}

        - pattern: 'trino.execution.executor<name=TaskExecutor><>(Blocked|Intermediate|Running|Waiting)Splits'
          name: 'trino_task_executor_splits'
          help: 'Number of splits in the labelled state'
          type: GAUGE
          labels:
            state: '$1'
        - pattern: 'trino.execution.executor<name=TaskExecutor><>TotalSplits'
          name: 'trino_task_executor_splits_total'
          help: 'Total number of splits'
          type: COUNTER

        # Intermediate and Leaf splits CPU, scheduled, wait and wall times histograms {{{
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(IntermediateSplitCpu|IntermediateSplitScheduled|IntermediateSplitWait|IntermediateSplitWall|LeafSplitCpu|LeafSplitScheduled|LeafSplitWait|LeafSplitWall)Time.(Avg|Min|Max)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_$2'
          help: '$2 $1 in seconds'
          type: GAUGE
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(IntermediateSplitCpu|IntermediateSplitScheduled|IntermediateSplitWait|IntermediateSplitWall|LeafSplitCpu|LeafSplitScheduled|LeafSplitWait|LeafSplitWall)Time.P(\d+)'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds'
          help: '$1 in seconds for the labelled quantile'
          type: GAUGE
          labels:
            quantile: '0.$2'
        - pattern: 'trino.execution.executor<name=TaskExecutor><>(IntermediateSplitCpu|IntermediateSplitScheduled|IntermediateSplitWait|IntermediateSplitWall|LeafSplitCpu|LeafSplitScheduled|LeafSplitWait|LeafSplitWall)Time.Count'
          valueFactor: 0.000001
          name: 'trino_task_executor_$1_seconds_count'
          help: '$1 in seconds count'
          type: COUNTER
        # }}}
        # }}}

        # Trino resource group stats {{{
        - pattern: 'trino.execution.resourcegroups<type=InternalResourceGroup, name=(.+)><>(HardConcurrencyLimit|MaxQueuedQueries|QueuedQueries|RunningQueries|WaitingQueuedQueries)'
          name: 'trino_resourcegroups_$2'
          help: '$2 for the labelled resource group name'
          type: GAUGE
          labels:
            group: '$1'
        # }}}

        # Trino cluster size {{{
        - pattern: 'trino.failuredetector<name=HeartbeatFailureDetector><>(Active|Failed|Total)Count'
          name: 'trino_failuredetector_$1_nodes'
          help: 'Number of $1 nodes observed by the failure detector'
          type: GAUGE
        # }}}

        # Trino memory stats {{{
        - pattern: 'trino.memory<name=ClusterMemoryManager><>Cluster(MemoryBytes|TotalMemoryReservation|UserMemoryReservation)'
          name: 'trino_cluster_memory_manager_$1_bytes'
          help: 'Cluster $1 in bytes'
          type: GAUGE
        - pattern: 'trino.memory<name=ClusterMemoryManager><>TotalAvailableProcessors'
          name: 'trino_cluster_memory_manager_available_processors'
          help: 'Cluster total available CPU cores'
          type: GAUGE
        - pattern: 'trino.memory<name=ClusterMemoryManager><>(NumberOfLeakedQueries|QueriesKilledDueToOutOfMemory)'
          name: 'trino_cluster_memory_manager_$1_total'
          help: 'Total $1'
          type: COUNTER

        # Cluster wide memory pools
        - pattern: 'trino.memory<type=ClusterMemoryPool, name=(.+)><>(AssignedQueries|BlockedNodes|Nodes)'
          name: 'trino_cluster_memory_pool_$2'
          help: '$1 in the labelled cluster memory pool'
          type: GAUGE
          labels:
            pool: '$1'
        - pattern: 'trino.memory<type=ClusterMemoryPool, name=(.+)><>(Free|Reserved|ReservedRevocable|Total)DistributedBytes'
          name: 'trino_cluster_memory_pool_$2_distributed_bytes'
          help: '$1 distributed memory in the labelled cluster memory pool in bytes'
          type: GAUGE
          labels:
            pool: '$1'

        # Local memory
        - pattern: 'trino.memory<type=MemoryPool, name=(.+)><>(Free|Max|Reserved|ReservedRevocable)Bytes'
          name: 'trino_memory_pool_$2_bytes'
          help: '$1 memory in the labelled memory pool in bytes'
          type: GAUGE
          labels:
            pool: '$1'
        # }}}

        # Trino metadata {{{
        - pattern: 'trino.metadata<name=DiscoveryNodeManager><>(Active|Inactive|ShuttingDown)NodeCount'
          name: 'trino_metadata_$1_nodes'
          help: 'Number of $1 nodes as seen by discovery service'
          type: GAUGE
        # }}}
        # Trino security stats {{{
        - pattern: 'trino.security<name=AccessControlManager><>(.+).TotalCount'
          name: 'trino_security_$1_count_total'
          help: 'Number of $1'
          type: COUNTER
        # }}}

        # SQL generator stats {{{
        - pattern: 'trino.sql.gen<name=(.+)><>(.+).(Hit|Miss)Rate'
          name: 'trino_sql_gen_$1_$2_$3_rate'
          type: GAUGE
        - pattern: 'trino.sql.gen<name=(.+)><>(.+).RequestCount'
          name: 'trino_sql_gen_$1_$2_request_count'
          type: COUNTER
        # }}}

        # SQL Planner stats {{{
        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>(Failures|Hits)'
          name: 'trino_sql_planner_$2_$4'
          type: COUNTER
          labels:
            rule: '$3'

        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>Time.(Avg|Min|Max)'
          valueFactor: 0.000001
          name: 'trino_sql_planner_$2_seconds_$4'
          type: GAUGE
          labels:
            rule: '$3'
        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>Time.P(\d+)'
          valueFactor: 0.000001
          name: 'trino_sql_planner_$2_seconds'
          type: GAUGE
          labels:
            rule: '$3'
            quantile: '0.$4'
        - pattern: 'trino.sql.planner.(.+)<name=(.+), rule=(.+)><>Time.Count'
          name: 'trino_sql_planner_$2_seconds_count'
          type: COUNTER
          labels:
            rule: '$3'
            # }}}
      startDelaySeconds: 30
